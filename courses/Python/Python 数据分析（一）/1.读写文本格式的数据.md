# 读写文本格式的数据


---

## 一、实验简介 ##

　　本实验将学习逐块读取文本文件、将数据写出到文本格式以及手工处理分隔符格式等知识。

　　输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用 Web API 操作网络资源

## 二、逐块读取文本文件 ##

由于本节实验需要使用到python的`lxml`库，所以先在环境中安装：

```
sudo apt-get update
sudo apt-get install python-lxml
```

　　Python 在文本和文件的处理以其简单的文本交互语法、直观的数据结构，以及诸如元组打包解包之类的便利功能，深受人们喜爱。

　　pandas 提供了一些用于将表格型数据读取为 DataFrame 对象的函数。

| 函数 | 说明 |
|------|------|
| read_csv | 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号 |
| read_table | 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符`\t` |
| read_fwf | 读取定宽列格式数据（也就是说，没有分隔符）|
| read\_clipboard | 读取剪贴板中的数据，可以看做 read_table 的剪贴板版。在将网页转换为表格时很有用 |

　　这些函数的选项可以划分为以下几大类：

* 索引：将一个或多个列当做返回的 DataFrame 处理，以及是否从文件、用户获取列名
* 类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等
* 日期解析：包括组合功，比如将分散在多个列中的日期时间信息组合成结果中的单个列
* 迭代：支持对大文件进行逐块迭代
* 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西（比如由成千上万个逗号隔开的数值数据）

　　类型推断（type inference）是这些函数中最重要的功能之一，我们不需要指定列的类型到底是数值、整数、布尔值，还是字符串。日期和其他自定义类型的处理需要多花点功夫才行。

　　首先我们来看一个以逗号分隔（我这里用的是 CSV 文件，同学们用 txt 文件是一样的）的文本文件：

![读取 CSV](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438136641810.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

　　我们也可以用`read_table`，只不过需要指定分隔符而已：

![read_table](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438136975990.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

　　在上面的例子中，我们在文本文件中写了标题行（即 message 那一行），但是不是所有文件都会有标题行，读入这种文件有两个办法。

![自定义索引值](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438137664949.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

![层次化索引](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438138125721.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

　　有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其他字符串）。对于这种情况，我们可以编写一个正则表达式来作为 read_table 的分隔符。

![正则表达式](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438139959090.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

　　上面的文本中 read_table 发现列名的数量比列的数量少1，于是推断第一列应该是 DataFrame 的索引。


　　我们现在就来看一看 read\_csv/read_table 函数的参数

| 参数 | 说明 |
|------|------|
| path | 表示文件系统位置、URL、文件型对象的字符串 |
| sep/delimiter | 用于对行中各字段进行拆分的字符序列或正则表达式 |
| header | 用作列名的行号。默认为0（第一行），如果没有 header 行就应该设置为 None |
| index_col | 用作行索引的列编号或列名。可以是单个名称/数字或由多个名称/数字组成的列表（层次化索引）|
| names | 用于结果的列名列表，结合 header= None |
| skiprows | 需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）
| na_values | 一组用于替换 NA 的值 |
| comment | 用于将注释信息从行尾拆分出去的字符（一个或多个）|
| parse_datas | 尝试将数据解析为日期，默认为 False，如果为 True，则尝试解析所有列。此外，还可以指定需要解析的一组列号或列名。如果列表的元素为列表或元组，就会将多个列组合到一起再进行日期解析工作 |
| keep\_data_col | 如果连接多列解析日期，则保持参与连接的列。默认为 False |
| converters | 由列号/列名跟函数之间的映射关系组成的字典。例如，{'foo' : f}会对 foo 列的所有值应用函数 f |
| dayfirst | 当解析有歧义的日期时，将其看做国际格式（例如，7/6/2012 → June 7,2012）。默认为 False |
| data_parser | 用于解析日期的函数 |
| nrows | 需要读取的行数（从文件开始处算起） |
| iterator |返回一个 TextParser 以便逐块读取文件 |
| chunksize | 文件块的大小（用于迭代） |
| skip_footer | 需要忽略的行数（从文件末尾处算起）|
| verbose | 打印各种解析器输出信息，比如“非数值列中缺失值的数量”等 |
| encoding | 用于 unicode 的文本编码格式 |
| squeeze | 如果数据经解析后仅含一列，则返回 Series |
| thousand | 千分位分隔符，如`,`或者`.` |


　　在处理很大文件或找出大文件中的参数集以便后续处理时，我们可能只想读取文件的一小部分或逐块对文件进行迭代。

![写入数据](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438151269643.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

![规定 nrows](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438151310970.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

![逐块读](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438151370167.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)


## 三、手工处理分隔符格式 ##

　　大部分存储在磁盘上的表格型数据都能用 `pandas.read_table`进行加载。然而，有时还是需要做一些手工处理。由于接收到含有畸形行的文件而使用 read_table 出毛病的情况并不少见。接下来我们来看一个例子：

![手工处理](https://dn-anything-about-doc.qbox.me/document-uid79144labid1237timestamp1438153598748.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

　　CSV文件的形式有很多。只需定义 csv.Dialect 的一个子类即可定义出新格式（如专门的分隔符、字符串引用约定、行结束符等）

```ipyton
In [61]: class my_dialect(csv.Dialect):
   ....:     lineterminator = '\n'
   ....:     delimiter = ';'
   ....:     quotechar = '"'
   ....:     
In [62]: reader = csv.reader(f, dialect=my_dialect, quoting = csv.QUOTE_ALL)

In [63]: reader
Out[63]: <_csv.reader at 0x1074c29f0>

In [64]: with open('mydata.csv','w') as f:
   ....:     writer = csv.writer(f,dialect = my_dialect,quoting = csv.QUOTE_ALL)
   ....:     writer.writerow(('one','two','three'))
   ....:     writer.writerow(('1','2','3'))
   ....:     writer.writerow(('4','5','6'))
   ....:     writer.writerow(('7','8','9'))
   ....:     

```
（mumu 发现仅截图，同学们学习起来不是特别方便，所以今后我会把代码写出来）

## 四、JSON 数据 ##

　　JSON（JavaScript Object Notation 的简称）已经成为通过 HTTP 请求在 Web 浏览器和其他应用程序之间发送数据的标准格式之一。它是一种比表格型文本格式（如 CSV）灵活得多的数据格式。如：

```ipython

obj = """
 { "name":"Limei",
   "places_lived":["China","UK","Germany"],
   "pet":null,
   "siblings":[{"name":"Liming","age":23,"pet":"Xiaobai"},
               {"name":"Lifang","age":33,"pet":"Xiaohei"}]
 }
 """
```

　　除其空值 null 和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON 非常接近于有效的 Python 代码。基本类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及 null。对象中所有的键都必须是字符串。许多 Python 库都可以读写 JSON 数据。我们将使用 json，因为它是构建与 Python 标准库中的。通过 json.loads 即可将 JSON 字符串转换成 Python 形式

```ipython
In [70]: import json

In [71]: result = json.loads(obj)

In [72]: result
Out[72]: 
{u'name': u'Limei',
 u'pet': None,
 u'places_lived': [u'China', u'UK', u'Germany'],
 u'siblings': [{u'age': 23, u'name': u'Liming', u'pet': u'Xiaobai'},
  {u'age': 33, u'name': u'Lifang', u'pet': u'Xiaohei'}]}
```
　　相反，json.dumps则将 Python 对象转换成 JSON 格式：

```ipython

In [78]: asjson = json.dumps(result)

In [79]: asjson
Out[79]: '{"pet": null, "siblings": [{"pet": "Xiaobai", "age": 23, "name": "Liming"}, {"pet": "Xiaohei", "age": 33, "name": "Lifang"}], "name": "Limei", "places_lived": ["China", "UK", "Germany"]}'

```
　　如何将（一个或一组）JSON 对象转换为DataFrame 或其他便于分析的数据结构呢？最简单方便的方式是：向 DataFrame 构造器传入一组 JSON 对象，并选取数据字段的子集。

```ipython
In [80]: siblings = DataFrame(result['siblings'],columns = ['name','age'])

In [81]: siblings
Out[81]: 
     name  age
0  Liming   23
1  Lifang   33
```


## 五、XML 和 HTML ##

　　Python 有许多可以读写 HTML 和 XML 格式数据的库。lxml 就是其中之一，它能够高效且可靠地解析大文件。lxml 有多个编程接口。首先我要用 lxml.html 处理 HTML，然后再用 lxml.objectify 做一些 XML 处理。

　　许多网站都将数据放到 HTML 表格中以便在浏览器中查看，但不能以一种更易于机器阅读的格式（如 JSON、HTML 或 XML）进行下载

　　首先，找到我们希望获取数据的 URL，利用 urllib2 将其打开，然后用 lxml 解析得到的数据流，如下所示：

```
In [4]: from lxml.html import parse

In [5]: from urllib2 import urlopen

In [6]: parsed = parse(urlopen('http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices'))

In [7]: doc = parsed.getroot()
```
　　通过这个对象，我们可以获取特定类型的所有 HTML 标签，比如含有所需数据的 table 标签。给这个简单的例子加点启发性，假设我们想得到该文档中所有的 URL 链接。HTML 中的链接是`a`标签。使用文档根节点的 findall 方法以及一个 XPath（对文档的“查询”的一种表示手段）：

```
 links = doc.findall('.//a')

In [9]: links[:3]
Out[9]: 
[<Element a at 0x10db065d0>,
 <Element a at 0x10db06628>,
 <Element a at 0x10db06680>]
```

　　但是这些是表示 HTML 元素的对象。要得到 URL 和链接文本，你必须使用各对象的 get 方法（针对 URL）和 text_content 方法（针对显示文本）：

```
In [11]: lnk = links[5]

In [12]: lnk
Out[12]: <Element a at 0x10db06788>

In [13]: lnk.get('href')
Out[13]: 'http://finance.yahoo.com/'

In [14]: lnk.text_content()
Out[14]: 'Finance'

```
　　因此编写下面这条列表推导式即可获取文档中的全部 URL：

```
In [16]: urls = [lnk.get('href') for lnk in doc.findall('.//a')]

In [17]: urls[:3]
Out[17]: 
['https://www.yahoo.com/',
 'https://mail.yahoo.com/?.intl=us&.lang=en-US&.src=ym',
 'https://search.yahoo.com/search']
```

　　现在，从文档中找出正确表格的办法就是反复试验了。有些网站会给目标表格加上一个`id`属性。mumu 试验了好久，终于在这个网站上找到了我们需要的表格：

```
In [70]: tables = doc.findall('.//table')

In [121]: #从70行试到了121行也是不容易的啊
In [122]: calls = tables[13]

In [123]: puts = tables[14]


In [124]: from pandas.io.parsers import TextParser
In [125]: #由于数值型数据任然是字符串格式
In [126]: #所以我们希望将部分列转换为浮点数格式
In [127]: #pandas 恰好就有一个 TextParser 类
In [128]: #用于自动类型转换

In [129]: def parse_options_data(table):
   .....:     rows = table.findall('.//tr')
   .....:     #每个表格都有一个标题行，然后才是数据行
   .....:     header = _unpack(rows[0],kind='th')
   .....:     #对于标题行，就是 th 单元格
   .....:     #数据行，就是 td 单元格
   .....:     data = [_unpack(r) for r in rows[1:]]
   .....:     return TextParser(data,names=header).get_chunk()
   .....: 

In [130]: call_data = parse_options_data(calls)
In [131]: put_data = parse_options_data(puts)

In [132]: call_data[:3]
Out[132]: 
           Date    Open    High     Low   Close      Volume  \
0  Jul 29, 2015  123.15  123.50  122.27  122.99  35,914,200   
1  Jul 28, 2015  123.38  123.91  122.55  123.38  33,448,900   
2  Jul 27, 2015  123.09  123.61  122.12  122.77  44,274,800   

   Adj Close*\n                              
0                                    122.99  
1                                    123.38  
2                                    122.77  

```

　　XML 是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。现在我们来学习另一种用于操作 XML 数据的接口，即 `lxml.objectify`

　　这里mumu 给同学们一个 xml 文件的内容，同学们用文本编译器生成一个 xml 文件即可：

```
<INDICATOR>  
<INDICATOR_SEQ>373889</INDICATOR_SEQ>  
<PARENT_SEQ></PARENT_SEQ>  
<AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>  
<INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>  
<DESCRIPTION>Percent of the time that escalators are operational  
systemwide. The availability rate is based on physical observations performed  
the morning of regular business days only. This is a new indicator the agency  
began reporting in 2009.</DESCRIPTION>  
<PERIOD_YEAR>2011</PERIOD_YEAR>  
<PERIOD_MONTH>12</PERIOD_MONTH>  
<CATEGORY>Service Indicators</CATEGORY>  
<FREQUENCY>M</FREQUENCY>  
<DESIRED_CHANGE>U</DESIRED_CHANGE>  
<INDICATOR_UNIT>%</INDICATOR_UNIT>  
<DECIMAL_PLACES>1</DECIMAL_PLACES>  
<YTD_TARGET>97.00</YTD_TARGET>  
<YTD_ACTUAL></YTD_ACTUAL>  
<MONTHLY_TARGET>97.00</MONTHLY_TARGET>  
<MONTHLY_ACTUAL></MONTHLY_ACTUAL>  
</INDICATOR>  
```
　　好了，接下来我们就进入 lxml.objectify 的学习吧

```
In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用：

In [4]: from lxml import objectify

In [5]: path = 'Performance_MNR.xml'

In [6]: parsed = objectify.parse(open(path))

In [7]: root = parsed.getroot()
In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用：

In [4]: from lxml import objectify

In [5]: path = 'Performance_MNR.xml'

In [6]: parsed = objectify.parse(open(path))

In [7]: root = parsed.getroot()


In [8]: data = []

In [9]: skip_fields = ['PARENT_SEQ''PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']

In [10]: for elt in root:
   ....:     el_data = {}
   ....:     for child in elt.getchildren():
   ....:         if child.tag in skip_fields:
   ....:             continue
   ....:         el_data[child.tag] = child.pyval
   ....:     data.append(el_data)
   ....:     


In [11]: data
Out[11]: 
[{'AGENCY_NAME': 'Metro-North Railroad',
  'CATEGORY': 'Service Indicators',
  'DESCRIPTION': 'Percent of the time that escalators are operational  \nsystemwide. The availability rate is based on physical observations performed  \nthe morning of regular business days only. This is a new indicator the agency  \nbegan reporting in 2009.',
  'FREQUENCY': 'M',
  'INDICATOR_NAME': 'Escalator Availability',
  'INDICATOR_UNIT': '%',
  'MONTHLY_ACTUAL': u'',
  'MONTHLY_TARGET': 97.0,
  'PARENT_SEQ': u'',
  'PERIOD_MONTH': 12,
  'PERIOD_YEAR': 2011,
  'YTD_ACTUAL': u'',
  'YTD_TARGET': 97.0}]

```


## 六、作业 ##

 　　好了，同学们今天的课程就到这里了。同学们自己找一找网上的资源，然后将它们的网页截取下来，将你想要的信息获取出来吧。